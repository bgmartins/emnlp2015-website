<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"
"http://www.w3.org/TR/html4/strict.dtd">
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
		<meta name="description" content="Invited speakers for the 2015 Conference on Empirical Methods on Natural Language Processing">
		<meta name="keywords" content="Invited Speakers, EMNLP2015, EMNLP, 2015, conference, empirical methods, natural language, Lisboa, Portugal, SIGDAT, ACL">
		<style type="text/css" media="all">
@import "css/style.css";
		</style>
		<title>
			Invited Speakers
		</title>
	</head>
	<body id="page">
		<p class="header">
			<a href="index.html">EMNLP 2015</a>: Conference on Empirical Methods in Natural Language Processing — September 17–21, 2015 — Lisboa, Portugal.
		</p>
		<div id="introduction">
			<div id="intro-left">
				<h2>
					<a href="index.html">emnlp<sub>2015</sub></a>
				</h2>
			</div>
		</div>
		<a name="top"/>
		<div id="content">
			<h1>
				Invited Speakers
			</h1>
			<h3>
				September 19, 2015
			</h3>
			<ul class="titles">
				<li><a href="#i1">Deep Learning of Semantic Representations</a> <span>Yoshua Bengio</span>
				</li>
			</ul>
			<h3>
				September 20, 2015
			</h3>
			<ul class="titles">
				<li><a href="#i2">Measuring How Elected Officials and Constituents Communicate</a> <span>Justin Grimmer</span>
				</li>
			</ul>
			<h1>
				Talk Descriptions
			</h1>

 <a name='i1'><h3><strong>Deep Learning of Semantic Representations</strong></h3></a>
 <h4>Yoshua Bengio</h4>
 <h4>September 19, 2015</h4>
 <h5>Abstract:</h5>
 <p>The core ingredient of deep learning is the notion of distributed representation.  This talk will start by explaining its theoretical advantages, in comparison with non-parametric methods based on counting frequencies of occurrence of observed tuples of values (like with n-grams). The talk will then explain how having multiple levels of representation, i.e., depth, can in principle give another exponential advantage. Neural language models have been extremely successful in recent years but extending their reach from language modeling to machine translation is very appealing because it forces the learned intermediate representations to capture meaning, and we found that the resulting word embeddings are qualitatively different. Recently, we introduced the notion of attention-based encoder-decoder systems, with impressive results on machine translation several language pairs and for mapping an image to a sentence, and these results will conclude the talk.</p>
 <h5>Yoshua Bengio, Full Professor, Université de Montréal:</h5>
  <ul class="titles">
  <li>
<p>
Yoshua Bengio received a PhD in Computer Science from McGill University, Canada in 1991. After two post-doctoral years, one at M.I.T. with Michael Jordan and one at AT&T Bell Laboratories with Yann LeCun and Vladimir Vapnik, he became professor at the Department of Computer Science and Operations Research at Université de Montréal. He is the author of two books and more than 200 publications, the most cited being in the areas of deep learning, recurrent neural networks, probabilistic learning algorithms, natural language processing and manifold learning. He is among the most cited Canadian computer scientists and is or has been associate editor of the top journals in machine learning and neural networks. Since '2000 he holds a Canada Research Chair in Statistical Learning Algorithms, since '2006 an NSERC Industrial Chair, since '2005 his is a Senior Fellow of the Canadian Institute for Advanced Research and since 2014 he co-directs its program focused on deep learning. He is on the board of the NIPS foundation and has been program chair and general chair for NIPS. He has co-organized the Learning Workshop for 14 years and co-created the new International Conference on Learning Representations. His current interests are centered around a quest for AI through machine learning, and include fundamental questions on deep learning and representation learning, the geometry of generalization in high-dimensional spaces, manifold learning, biologically inspired learning algorithms, and challenging applications of statistical machine learning.
</p>
</li>
 </ul>
 <p><a href='#top'>top of the page</a></p>  

<a name='i2'><h3><strong>Measuring How Elected Officials and Constituents Communicate</strong></h3></a>
 <h4>Justin Grimmer</h4>
 <h4>September 20, 2015</h4>
 <h5>Abstract:</h5>
 <p>
This talk will show how elected officials use communication to cultivate support with constituents, how constituents express their views to elected officials, and why biases in both kinds of communication matter for political representation. To demonstrate the bias and its effects, I propose to use novel collections of political texts and new text as data methods. Using the new data and methods, I will show how the incentives of communication contribute to perceptions of an angry public and vitriolic politicians. Among elected officials, the ideologically extreme members of Congress disproportionately participate in policy debates, resulting in political debates that occur between the most extreme members of each party. Among constituents, the most ideologically extreme and angry voters disproportionately contact their member of Congress, creating the impression of a polarized and vitriolic public. The talk will explain how the findings help us to understand how representation occurs in American politics, while also explaining how computational tools can help address questions in the social sciences. 	
 </p>
 <h5>Justin Grimmer, Associate Professor, Stanford University:</h5>
  <ul class="titles">
  <li>
<p>
Justin Grimmer is an associate professor of political science at Stanford University. His research examines how representation occurs in American politics using new statistical methods. His first book Representational Style in Congress: What Legislators Say and Why It Matters (Cambridge University Press, 2013) shows how senators define the type of representation they provide constituents and how this affects constituents' evaluations and was awarded the 2014 <i>Richard Fenno Prize</i>.  His second book The Impression of Influence: How Legislator Communication and Government Spending Cultivate a Personal Vote (Princeton University Press, 2014 with Sean J. Westwood and Solomon Messing) demonstrates how legislators ensure they receive credit for government actions.  His work has appeared in the <i>American Political Science Review, American Journal of Political Science, Journal of Politics, Political Analysis, Proceedings of the National Academy of Sciences, Regulation and Governance,</i> and <i>Poetics</i>. 
</p>
</li>
</ul>
 <p><a href='#top'>top of the page</a></p>  -->
 
</div>
		<div id="footer">
			<div>
				<p>
					Website design <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/">by-nc-sa</a> © 2008-2015 <a href="http://fran.io/">Francesco Figari</a>
				</p>
			</div>
		</div>
	</body>
</html>
